---
title: "Time Varying Breakdown"
output: html_document
date: "2025-02-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = here::here())
```
# Packages and Data
```{r load packages, echo = FALSE}
library(tidyverse)
library(here)
library(litterfitter)
library(ggsci)
library(doSNOW)
library(parallel)
library(foreach)
source(here("litterfitter_prediction_functions.R"))

```

# Load Data
```{r}
coweeta_df <- read_csv(here("wholestream_breakdown_calculations_timeKexploration.csv"))|>
  mutate(mass_prop = pct.mass.remain/100, .before = sample.id)|>
  filter(mass_prop >0)|> #filters out two NAs, one with negative mass remaining
  group_by(year, stream, block, mesh.type, species)|>
  mutate(cohort_id = cur_group_id(), .before = sample.id)

length(unique(coweeta_df$cohort_id))


```
This dataset has `r length(unique(coweeta_df$cohort_id))` time series. 

# Visualization

```{r}
coweeta_df|>
  filter(species == "ACER")|>
  ggplot(aes(x = days.incubated, y = mass_prop, color = mesh.type, shape = as_factor(block), group = cohort_id))+
  geom_point()+
  scale_color_jco()+
  facet_wrap(year~stream)+
  scale_y_continuous(breaks = c(0,0.2,0.4,0.6,0.8,1))+
  theme_bw()

coweeta_df|>
  filter(species == "RHODO")|>
  ggplot(aes(x = days.incubated, y = mass_prop, color = mesh.type, shape = as_factor(block), group = cohort_id))+
  geom_point()+
  scale_color_jco()+
  facet_wrap(year~stream)+
  scale_y_continuous(breaks = c(0,0.2,0.4,0.6,0.8,1), limits = c(0,1))+
  theme_bw()

blah <- coweeta_df|>
  filter(species == "RHODO", cohort_id == 106)|>
  select(mass_prop, days.incubated, mesh.type, block, cohort_id)


```


# Fit models
This employs the use of the litterfitter package which optimizes model parameters for fitting non-linear analytical decomposition functions to mass loss data. 


```{r}

if(file.exists(here("coweeta_litter_fits.rds"))){
  
  litter_fits <- readRDS(here("coweeta_litter_fits.rds"))

}else{

#set up data
nested_data <- coweeta_df|>
    select(cohort_id, days.incubated, mass_prop)|>
    #filter(cohort_id %in% seq(1:2))|> #for testing
    group_by(cohort_id)|> # needs group_by()|>nest() structure... nest_by() doesn't work!
    nest()|>
  ungroup()|>
  mutate(batch_number = row_number())

# Definitions for use in parallel loops
n_data_batches <- nrow(nested_data)
cohort_names <- nested_data|>pull(cohort_id)
n_iters <- 999

pb <- txtProgressBar(max = n_data_batches, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)

# Set parallel processing
cl <- makePSOCKcluster(14) 
registerDoSNOW(cl)


# Loop
litter_fits <- foreach(i = 1:nrow(nested_data), .combine = 'rbind', .options.snow = opts, 
        .packages = c("tidyverse", "litterfitter", "future"), .inorder = TRUE#, .final = function(x) setNames(x, cohort_names)
        ) %dopar% {
          
          # filter to cohort data and unnest into normal data frame
      df <- nested_data|>filter(batch_number == i)|>unnest()
        
       cohort_id <- df|>first()|>pull(cohort_id)
          #negative exponential fit
          negexp_fit <- fit_litter(time = df$days.incubated,  
                                              mass.remaining = df$mass_prop,
                                              upper = c(2), 
                                              lower = c(1e-08),
                                              model = "neg.exp",
                                              iters = n_iters/10) # need far fewer iterations for only one parameter
          
          #weibull fit
          weibull_fit <- fit_litter(time = df$days.incubated, 
                                              mass.remaining = df$mass_prop,
                                              lower = c(0.1,0.001),#beta, then alpha 
                                              upper = c(25000,100),#beta, then alpha 
                                              model = "weibull",
                                              iters = n_iters)
        
          #discrete parallel fit
          discpar_fit <- fit_litter(time = df$days.incubated, 
                                              mass.remaining = df$mass_prop,
                                              upper = c(1, 50, 50), #A, K1, K2 
                                              lower = c(0, 1e-12, 1e-17), #A, K1, K2
                                              model = "discrete.parallel",
                                              iters = n_iters)
        
          #discrete series fit
           discser_fit <- fit_litter(time = df$days.incubated, 
                                              mass.remaining = df$mass_prop,
                                              upper = c(1, 50, 2), #R, K1, K2
                                              lower = c(0, 1e-04, 1e-20), #R, K1, K2
                                              model = "discrete.series",
                                              iters = n_iters)
           
           # continuous quality/continuous exponential fit
            contqual_fit = fit_litter(time = df$days.incubated, 
                                              mass.remaining = df$mass_prop,
                                              upper = c(10^6, 50), #b,a
                                              lower = c(1e-04, 0.01), #b,a
                                              model = "cont.quality",
                                              iters = n_iters)
            
            # Not actually sure what the name of this is, but estimates a lower mass limit at which decomposition ceases
            negexplim_fit = fit_litter(time = df$days.incubated, 
                                              mass.remaining = df$mass_prop,
                                              upper = c(10, 50, 1),
                                              lower = c(1e-06, 1e-02, 1e-20),
                                              model = "neg.exp.limit",
                                              iters = n_iters)
          
            # Package up results for export
            tibble(cohort_id = cohort_id,
                   negexp_fit = list(negexp_fit), 
                    weibull_fit = list(weibull_fit),
                    discpar_fit = list(discpar_fit),
                    discser_fit = list(discser_fit),
                    contqual_fit = list(contqual_fit),
                    negexplim_fit = list(negexplim_fit))
            
   
        }     
stopCluster(cl)

# save to file
saveRDS(litter_fits, file = here("coweeta_litter_fits.rds")) 

# Loops took about 7 minutes with 14 cores
}

```

```{r extract and tidy model fits}
tidy_fits <- 
  litter_fits|>
  mutate(negexp_converge_code = map_dbl(negexp_fit, ~pluck(., "optimFit", "convergence")),
         negexp_k = map_dbl(negexp_fit, ~pluck(., "optimFit", "par", 1)),
         negexp_AICc = map_dbl(negexp_fit, ~pluck(., "fitAICc")),
         negexp_logLik = map_dbl(negexp_fit, ~pluck(., "logLik")),
         negexp_BIC = map_dbl(negexp_fit, ~pluck(., "fitBIC")),
         negexp_pred = map(negexp_fit, ~pluck(., "predicted"))
  )|>
  mutate(weibull_converge_code = map_dbl(weibull_fit, ~pluck(., "optimFit", "convergence")),
         weibull_beta = map_dbl(weibull_fit, ~pluck(., "optimFit", "par", 1)),
         weibull_alpha = map_dbl(weibull_fit, ~pluck(., "optimFit", "par", 2)),
         weibull_AICc = map_dbl(weibull_fit, ~pluck(., "fitAICc")),
         weibull_logLik = map_dbl(weibull_fit, ~pluck(., "logLik")),
         weibull_BIC = map_dbl(weibull_fit, ~pluck(., "fitBIC")),
         weibull_pred = map(weibull_fit, ~pluck(., "predicted"))
         )|>
  mutate(discpar_converge_code = map_dbl(discpar_fit, ~pluck(., "optimFit", "convergence")),
         discpar_a = map_dbl(discpar_fit, ~pluck(., "optimFit", "par", 1)),
         discpar_k1 = map_dbl(discpar_fit, ~pluck(., "optimFit", "par", 2)),
         discpar_k2 = map_dbl(discpar_fit, ~pluck(., "optimFit", "par", 3)),
         discpar_AICc = map_dbl(discpar_fit, ~pluck(., "fitAICc")),
         discpar_logLik = map_dbl(discpar_fit, ~pluck(., "logLik")),
         discpar_BIC = map_dbl(discpar_fit, ~pluck(., "fitBIC")),
         discpar_pred = map(discpar_fit, ~pluck(., "predicted"))
         )|>
  mutate(discser_converge_code = map_dbl(discser_fit, ~pluck(., "optimFit", "convergence")),
         discser_r = map_dbl(discser_fit, ~pluck(., "optimFit", "par", 1)),
         discser_k1 = map_dbl(discser_fit, ~pluck(., "optimFit", "par", 2)),
         discser_k2 = map_dbl(discser_fit, ~pluck(., "optimFit", "par", 3)),
         discser_AICc = map_dbl(discser_fit, ~pluck(., "fitAICc")),
         discser_logLik = map_dbl(discser_fit, ~pluck(., "logLik")),
         discser_BIC = map_dbl(discser_fit, ~pluck(., "fitBIC")),
         discser_pred = map(discser_fit, ~pluck(., "predicted"))
         )|>
  mutate(contqual_converge_code = map_dbl(contqual_fit, ~pluck(., "optimFit", "convergence")),
         contqual_b = map_dbl(contqual_fit, ~pluck(., "optimFit", "par", 1)),
         contqual_a = map_dbl(contqual_fit, ~pluck(., "optimFit", "par", 2)),
         contqual_AICc = map_dbl(contqual_fit, ~pluck(., "fitAICc")),
         contqual_logLik = map_dbl(contqual_fit, ~pluck(., "logLik")),
         contqual_BIC = map_dbl(contqual_fit, ~pluck(., "fitBIC")),
         contqual_pred = map(contqual_fit, ~pluck(., "predicted"))
         )|>
  mutate(negexplim_converge_code = map_dbl(negexplim_fit, ~pluck(., "optimFit", "convergence")),
         negexplim_k = map_dbl(negexplim_fit, ~pluck(., "optimFit", "par", 1)),
         negexplim_a = map_dbl(negexplim_fit, ~pluck(., "optimFit", "par", 2)),
         negexplim_b = map_dbl(negexplim_fit, ~pluck(., "optimFit", "par", 3)),
         negexplim_AICc = map_dbl(negexplim_fit, ~pluck(., "fitAICc")),
         negexplim_logLik = map_dbl(negexplim_fit, ~pluck(., "logLik")),
         negexplim_BIC = map_dbl(negexplim_fit, ~pluck(., "fitBIC")),
         negexplim_pred = map(negexplim_fit, ~pluck(., "predicted"))
  )|>
  select(-c(negexp_fit, weibull_fit, discser_fit, discpar_fit, contqual_fit, negexplim_fit))|>
  left_join(coweeta_df|>
  select(cohort_id, mass_prop, block, species, stream, year, mesh.type)|>
  group_by(cohort_id)|>
  slice_head(n =1))

```


```{r}
tidy_fits|>
  ggplot(aes(x = weibull_alpha))+geom_histogram()+
  facet_wrap(mesh.type~species, scales = "free")

tidy_fits|>
  filter(weibull_alpha <3)|>
  ggplot(aes(x = weibull_alpha))+geom_histogram()

```

```{r parsimony comparisons}

mod_gof <- tidy_fits|>
  #reshape for usability
  select(cohort_id, contains(c("AIC", "AICc", "BIC", "logLik")))|>
  pivot_longer(cols = -c(cohort_id), names_to = c("model", "gof_metric"), names_sep = "_", values_to = "value")|>
  filter(model %in% c("negexp","discpar","weibull"))|>
  group_by(cohort_id, gof_metric)|>
  mutate(best_in_cohort = case_when(gof_metric == "logLik" & value == max(value) ~ 1, #select highest when logLik
                                    gof_metric != "logLik" & value == min(value) ~ 1, # select lowest when AIC/BIC
                                    .default = 0))|>
    mutate(model_type = ifelse(model != "negexp", "non-constant", "negexp"))

```

```{r}

fit_comparisons <- mod_gof|>
  filter(best_in_cohort==1)|>
  group_by(gof_metric, model)|>
  count()|>
  ungroup()|>group_by(gof_metric)|>mutate(per = n/sum(n))

fit_comparisons_type <- mod_gof|>
  filter(best_in_cohort==1)|>
  group_by(gof_metric, model_type)|>
  count()|>
  ungroup()




n_time_series <- length(unique(tidy_fits$cohort_id))

fit_counts.p <- fit_comparisons|>
  filter(gof_metric != "logLik")|>
  mutate(model = fct_relevel(model, "weibull", "negexp", after = Inf),
         model = fct_recode(model,Weibull = "weibull", NegExp = "negexp", `Continuous Quality` = "contqual", `Discrete Parallel` = "discpar", 
                            `Discrete Series` = "discser"))|>
  ggplot(aes(x = gof_metric, y = n, fill = model), alpha =  1)+
  geom_bar(stat = "identity")+
  scale_fill_jco()+
  scale_y_continuous(breaks = c(0,50,100, n_time_series))+
  guides(fill = guide_legend("Model"))+
  xlab("Model Comparison Metric")+
  ylab("Count")+
  theme_classic()+
  theme(axis.title = element_text(face = "bold", size = 16), axis.text = element_text(size = 14, color = "black"),
        legend.title = element_text(size = 14), legend.text = element_text(size = 12))

fit_counts.p


```

```{r}
newpreds_df <- tidy_fits|>
  rowwise()|>
  mutate(negexp_newpreds = list(predict_negexp(time = newtimes, k = negexp_k)),
         discser_newpreds = list(predict_discser(time = newtimes, r = discser_r, k1 = discser_k1, k2 = discser_k2)),
         discpar_newpreds = list(predict_discpar(time = newtimes, a = discpar_a, k1 = discpar_k1, k2 = discpar_k2)),
         contqual_newpreds = list(predict_contqual(time = newtimes, a = contqual_a, b = contqual_b)),
         weibull_newpreds = list(predict_weibull(time = newtimes, beta = weibull_beta, alpha = weibull_alpha)),
         )|>
  select(cohort_id, ends_with("newpreds"))|>
  mutate(days.incubated = list(newtimes))|>
  group_by(cohort_id)|>
  unnest(cols = c(days.incubated, ends_with("newpreds")))|>
  pivot_longer(cols = ends_with("newpreds"), values_to = ".pred", names_to = "model")|>
  mutate(model = str_remove(model, "_newpreds"))

preds_all_weibull <- coweeta_df|>select(cohort_id,days.incubated)|>
  left_join(newpreds_df|>filter(model == "weibull"))|>
  left_join(tidy_fits|>select(cohort_id, weibull_alpha))
  

weibull_preds_df<-coweeta_df|>
  select(cohort_id, days.incubated, mass_prop)|>
  group_by(cohort_id)|>
  group_modify(~add_row(., days.incubated = 0, mass_prop = 0))|>
  complete(days.incubated = full_seq(days.incubated, 1))|>
  left_join(newpreds_df|>filter(model == "weibull"))|>left_join(tidy_fits|>select(weibull_alpha,cohort_id))


weibulls.p <- ggplot(data = weibull_preds_df|>select(-model)|>filter(weibull_alpha < 30))+
  geom_line(aes(x = days.incubated, y = .pred, group = cohort_id, color = weibull_alpha), alpha = 0.5)+
  scale_y_continuous(limits = c(0, 1), breaks = c(0,0.25,0.5,0.75,1.0))+
  scale_x_continuous(limits = c(0,max(preds_all_weibull$days.incubated)), breaks = c(0,50,100,150,200, max(preds_all_weibull$days.incubated)))+
  #scale_color_jco()+
  scale_color_gradient2(trans = "log", low = "red", mid = "grey", high = "black", breaks = c(0.1,0.3,1,3))+
  #scale_color_gradient2()+
  guides(color = guide_colorbar(title = expression(Weibull~alpha)))+
  xlab("Day")+
  ylab("Proportional Mass Remaining")+
  theme_classic()+
  theme(panel.background = element_rect(fill = "#F7F9F9"),
        legend.title = element_text(size = 20), legend.text = element_text(size = 16), 
        legend.position = c(0.9,0.78), legend.background = element_blank())
weibulls.p

weibulls.p + scale_y_log10()

```

```{r bootstrap alpha}



#set up data
nested_data <- litter_fits|>
    select(cohort_id, weibull_fit)|>
    filter(cohort_id %in% seq(1:4))|> #for testing
    group_by(cohort_id)|> # needs group_by()|>nest() structure... nest_by() doesn't work!
    nest()|>
  ungroup()|>
  mutate(batch_number = row_number())

# Definitions for use in parallel loops
n_data_batches <- nrow(nested_data)
cohort_names <- nested_data|>pull(cohort_id)
n_iters <- 999

pb <- txtProgressBar(max = n_data_batches, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)

# Set parallel processing
cl <- makePSOCKcluster(4) 
registerDoSNOW(cl)

# Loop
weibull_alpha_bs <- foreach(i = 1:nrow(nested_data), .combine = 'rbind', .options.snow = opts, 
        .packages = c("tidyverse", "litterfitter"), .inorder = TRUE#, .final = function(x) setNames(x, cohort_names)
        ) %dopar% {
          
          # filter to cohort data and unnest into normal data frame
      df <- nested_data|>filter(batch_number == i)|>unnest()
  
       cohort_id <- df|>first()|>pull(cohort_id)
       weibull_fit <- df|>pluck("weibull_fit",1)
       

          
            weibull_bs <- litterfitter::bootstrap_parameters(x = weibull_fit, nboot = n_iters, lower = c(0.1,0.001), 
                                               upper = c(25000,100)) #beta, then alpha - same bounds used for main litter fitting exercise
            
           alpha <- weibull_bs[, 2] # [[,1] is beta, [,3] is the steady state
           beta <- weibull_bs[, 1]
  
          qfs <- quantile(alpha, probs = c(0.025, 0.975))
       
          
       tibble(cohort_id = cohort_id,
              weibull_alpha_dist = list(alpha),
              weibull_beta_dist = list(beta),
              weibull_alpha_025 = qfs[1],
              weibull_alpha_975 =  qfs[2])
      
    
            
   
        }     
stopCluster(cl)

weibull_alpha_bs2 <- weibull_alpha_bs|>
  rowwise()|>
  mutate(weibulla_overlap_1 = between(1, weibull_alpha_025,weibull_alpha_975))

# save to file
#weibull_alpha_bs2|>write_csv(file = here("data/derived_data/DetNut_weibullalpha_bootstraps.csv")) 

```

```{r test plot bs}
blah <- weibull_alpha_bs|>
  left_join(coweeta_df2|>
              group_by(cohort_id, species, block, stream, year, mesh.type)|>
              summarize(max_time = max(time)))|>
  slice_head(n = 1)|>
  unnest(cols = c(weibull_alpha_dist, weibull_beta_dist))|>
  rowwise()|>
  mutate(times = list(seq(0, max_time, by = 1)),
         preds = list(predict_weibull(time = times, alpha = weibull_alpha_dist, beta = weibull_beta_dist)))|>
  mutate(index = row_number())

ggplot(aes(x = unlist(times), y = unlist(preds), group = index))+
  geom_line()



```

```{r brms fits}

# Fit Bayesian litter fits

# Load data
coweeta_df2 <- coweeta_df|>
  filter(species != "WOOD")|>
  rename(mass = "mass_prop", time = "days.incubated")

# Set test data
series_ids <- unique(coweeta_df2$cohort_id)
series_ids <- series_ids[1:4] #testing


# Set priors and formulae

## K

k_bf <- bf(mass ~ exp(-k*time), k ~1, nl = TRUE)
k_priors <- prior(lognormal(-6,2), nlpar = "k", lb = 0)

## Weibull
weibull_bf <- bf(mass ~ exp(-(time/beta)^alpha), beta + alpha ~1, nl = TRUE)
weibull_priors <- prior(gamma(1,7), nlpar = "alpha", lb = 0)+
  prior(lognormal(5.5,1), nlpar = "beta", lb = 0)

## Discrete Parallel
discpar_bf <- bf(mass ~ a*exp(-k1 * time) + (1 - a) * exp(-k2 * time),
                 a + k1 + k2 ~ 1, nl = TRUE)


## Discrete Series



# Fitting loops
nested_data_brm <- coweeta_df2|>
  filter(cohort_id %in% series_ids)|>
  select(cohort_id, time, mass)|>
  #filter(cohort_id %in% seq(1:38))|> #for testing
  group_by(cohort_id)|> # needs group_by()|>nest() structure... nest_by() doesn't work!
  nest()|>
  ungroup()|>
  mutate(batch_number = row_number())

# Definitions for use in parallel loops
n_data_batches <- nrow(nested_data_brm)
cohort_names <- nested_data_brm|>pull(cohort_id)

pb <- txtProgressBar(max = n_data_batches, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)

# Set parallel processing
cl <- makePSOCKcluster(4)
registerDoSNOW(cl)
tic()
# Loop
weibull_summaries <- foreach(i = 1:nrow(nested_data), .combine = 'rbind', .options.snow = opts,
                         .packages = c("tidyverse", "brms", "here", "tidybayes"), .inorder = FALSE
) %dopar% {

  # filter to cohort data and unnest into normal data frame
  df <- nested_data_brm|>filter(batch_number == i)|>unnest()
  cohort_id <- df|>first()|>pull(cohort_id)
  file_name <- paste0("cohort_", cohort_id)

  fit <- brm(weibull_bf,
             prior = weibull_priors,
             data = df,
             chains = 4, cores = 1, iter = 4000,
             warmup = 500, control= list(adapt_delta = 0.99),
             file = paste0("analysis/data/derived_data/brms_fits/weibull/",
                           file_name)
  )

  summarise_draws(fit)|>
    tibble(cohort_id = cohort_id,
           model_type = "Weibull")|>
      relocate(cohort_id, model_type)


}
stopCluster(cl)
toc()
# save model objects, summarize parameter HDIs, diagnostic values, to table
fit <- readRDS(here("analysis/data/derived_data/brms_fits/weibull/cohort_16.Rds"))
summary(fit)
summarise_draws(fit)
```

# Test hypotheses

- Coarse and fine mesh have similar early stage decomposition
- Fragmentation 
